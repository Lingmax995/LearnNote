# 线性回归介绍

回归（regression）是能为**一个或多个自变量**与**因变量**之间关系建模的一类方法

## 基本元素

在机器学习的术语中，数据集称为训练数据集（training data set） 或训练集（training set）。 
 
每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。
 
我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目（target）。

预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。

### 线性模型

线性假设是指**目标**可以表示为**特征**的加权和，如下面的式子

<div align="center">
price = W<sub>a</sub>*a+W<sub>b</sub>*b+b
</div>

W<sub>a</sub>称为权重，权重决定了每个特征对我们预测值的影响

 
b称为偏置（bias）、偏移量（offset）或截距（intercept）

给定一个数据集，我们的目标是寻找模型的权重w和偏置b

在机器学习领域，我们通常使用的是高维数据集，建模时采用**线性代数**表示法会比较方便。


则
对于特征集合**X**，预测值ŷ可以通过矩阵-向量乘法表示为

<div align="center">
ŷ=**Xw**+b
</div>
这个过程中的求和将使用广播机制。

给定训练数据特征X和对应的已知标签y，线性回归的目标是找到一组权重向量w和偏置b

当给定从X的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小

### 损失函数

损失函数（loss function）能够量化目标的实际值与预测值之间的差距。


通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。

回归问题中最常用的损失函数是平方误差函数。

###  解析解

与在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解

将损失关于w的导数设为0，得到解析解：

<div align="center">
w<sup>*</sup>=(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y
</div>


注：
这个公式是线性回归中最小二乘法（Ordinary Least Squares, OLS）的解析解，我们来详细解释它：

📌 公式回顾

将损失函数对参数 $\mathbf{w}$ 求导并设为 0，得到：

$$
\mathbf{w}^ = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

这是线性回归模型的最优权重向量（即参数估计值）。

🔍 各符号含义
$\mathbf{w}^$：最优参数向量（待求解）
$\mathbf{X}$：设计矩阵（输入数据），形状为 $n \times d$
$n$：样本数量
$d$：特征维度
每一行是一个样本的特征向量
$\mathbf{y}$：目标变量向量，形状为 $n \times 1$
每个元素是对应样本的真实标签
$\mathbf{X}^\top$：$\mathbf{X}$ 的转置矩阵，形状为 $d \times n$
$(\mathbf{X}^\top \mathbf{X})^{-1}$：$\mathbf{X}^\top \mathbf{X}$ 的逆矩阵，形状为 $d \times d$

🧠 推导思路（简要）

我们希望找到一个线性模型：
$$
\hat{\mathbf{y}} = \mathbf{X} \mathbf{w}
$$
使得预测值 $\hat{\mathbf{y}}$ 尽可能接近真实值 $\mathbf{y}$。

定义均方误差损失函数：
$$
L(\mathbf{w}) = \ \mathbf{y} - \mathbf{X}\mathbf{w}\ ^2
$$

对 $\mathbf{w}$ 求导并令导数为 0：
$$
\frac{\partial L}{\partial \mathbf{w}} = -2\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\mathbf{w}) = 0
$$

整理得：
$$
\mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}
$$

两边同时左乘 $(\mathbf{X}^\top \mathbf{X})^{-1}$，得到：
$$
\mathbf{w}^ = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

✅ 这个公式的含义
它给出了直接计算最优参数的方法，无需迭代优化。
是闭式解（closed-form solution），在数据规模不大时非常高效。

⚠️ 使用条件

1. $\mathbf{X}^\top \mathbf{X}$ 必须可逆
即特征之间不能完全线性相关（无多重共线性）
若不可逆，可用正则化（如 Ridge 回归）或伪逆（Moore-Penrose 逆）

2. 数据量适中
当样本数 $n$ 很大时，矩阵求逆计算复杂度高（$O(d^3)$），此时梯度下降更优

💡 应用场景
线性回归建模
特征工程中的基线模型
教学与理论推导

📌 总结

$$
\mathbf{w}^ = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

是线性回归的解析解，通过最小化平方误差损失，直接求出最优参数。它是统计学习和机器学习中最经典的公式之一。


### 随机梯度下降

本书中我们用到一种名为梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。

 它通过不断地在损失函数递减的方向上更新参数来降低误差。

 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）


这个公式描述的是随机梯度下降算法（Stochastic Gradient Descent, SGD）在线性回归模型中的具体更新规则。我们来逐步解释它的含义和推导过程。

📌 公式回顾

对于一个线性模型：
$$
\hat{y}^{(i)} = \mathbf{w}^\top \mathbf{x}^{(i)} + b
$$
其中：
$\mathbf{w}$：权重向量
$b$：偏置项（截距）
$\mathbf{x}^{(i)}$：第 $i$ 个样本的特征向量
$y^{(i)}$：第 $i$ 个样本的真实标签

使用平方损失函数：
$$
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2
$$

然后，SGD 的参数更新规则为：

$$
\begin{aligned}
\mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{ B } \sum_{i \in B} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) \\
&= \mathbf{w} - \frac{\eta}{ B } \sum_{i \in B} \mathbf{x}^{(i)} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)
\end{aligned}
\tag{3.1.10}
$$

$$
\begin{aligned}
b &\leftarrow b - \frac{\eta}{ B } \sum_{i \in B} \partial_b l^{(i)}(\mathbf{w}, b) \\
&= b - \frac{\eta}{ B } \sum_{i \in B} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)
\end{aligned}
$$

🔍 各符号含义

符号 含义
------ ------
$\mathbf{w}$ 模型权重向量
$b$ 偏置项（截距）
$\mathbf{x}^{(i)}$ 第 $i$ 个样本的特征向量
$y^{(i)}$ 第 $i$ 个样本的真实标签
$B$ 当前批次（batch）的样本索引集合
$ B $ 批次大小（batch size）
$\eta$ 学习率（learning rate）
$\partial_{\mathbf{w}} l^{(i)}$ 损失函数对 $\mathbf{w}$ 的偏导数
$\partial_b l^{(i)}$ 损失函数对 $b$ 的偏导数

✅ 推导过程（简要）
1. 对 $\mathbf{w}$ 求导

$$
\partial_{\mathbf{w}} l^{(i)} = (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}) \cdot \mathbf{x}^{(i)}
$$

所以：
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \cdot \frac{1}{ B } \sum_{i \in B} (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}) \cdot \mathbf{x}^{(i)}
$$
2. 对 $b$ 求导

$$
\partial_b l^{(i)} = (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)})
$$

所以：
$$
b \leftarrow b - \eta \cdot \frac{1}{ B } \sum_{i \in B} (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)})
$$

🧠 算法思想

1. 初始化：随机初始化 $\mathbf{w}$ 和 $b$
2. 采样：从数据集中随机抽取一个小批量样本（mini-batch），记为 $B$
3. 计算梯度：对这批样本计算损失函数关于 $\mathbf{w}$ 和 $b$ 的平均梯度
4. 更新参数：沿负梯度方向更新参数
5. 重复：不断迭代，直到收敛

📌 关键点总结
随机性：每次只用一小部分数据更新参数，加快训练速度
近似梯度：使用小批量样本的平均梯度近似整体梯度
学习率 $\eta$：控制更新步长，过大可能不收敛，过小收敛慢
批大小 $ B $：通常为 32、64、128 等，影响稳定性和效率

💡 应用场景
线性回归
逻辑回归
神经网络训练（作为基础优化算法）

📌 总结

这个公式是随机梯度下降（SGD）在线性回归中的具体实现。它通过小批量样本估计梯度，并以负梯度方向更新参数，从而逐步逼近最优解。它是现代机器学习中最基础、最常用的优化算法之一。
